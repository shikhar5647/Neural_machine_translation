{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd77a235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.1.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (13 kB)\n",
      "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyahocorasick-2.1.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.4/104.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f52df299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (23.1.21)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (15.0.6.1)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow)\n",
      "  Downloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (2.2.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (0.30.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/palashdas/.local/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/palashdas/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/palashdas/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/palashdas/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/palashdas/.local/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/palashdas/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/palashdas/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/palashdas/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/palashdas/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/palashdas/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/palashdas/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/palashdas/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.14.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/palashdas/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.1.0)\n",
      "Using cached tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.6 MB)\n",
      "Using cached numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: typing-extensions, protobuf, numpy, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.0\n",
      "    Uninstalling numpy-1.21.0:\n",
      "      Successfully uninstalled numpy-1.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sqlalchemy 2.0.26 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "datasets 2.12.0 requires tqdm>=4.62.1, but you have tqdm 4.59.0 which is incompatible.\n",
      "evaluate 0.4.0 requires tqdm>=4.62.1, but you have tqdm 4.59.0 which is incompatible.\n",
      "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
      "pytorch-lightning 1.6.5 requires protobuf<=3.20.1, but you have protobuf 4.25.3 which is incompatible.\n",
      "streamlit 1.28.2 requires pandas<3,>=1.3.0, but you have pandas 1.0.5 which is incompatible.\n",
      "vit-pytorch 1.0.2 requires einops>=0.6.0, but you have einops 0.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.3 protobuf-4.25.3 tensorflow-2.13.1 typing-extensions-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d34d864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: typing-extensions in /home/palashdas/.local/lib/python3.8/site-packages (4.5.0)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: tqdm in /home/palashdas/.local/lib/python3.8/site-packages (4.59.0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: decorator in /home/palashdas/.local/lib/python3.8/site-packages (5.1.1)\n",
      "Requirement already satisfied: pandas in /home/palashdas/.local/lib/python3.8/site-packages (1.0.5)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: einops in /home/palashdas/.local/lib/python3.8/site-packages (0.4.1)\n",
      "Collecting einops\n",
      "  Using cached einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/palashdas/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/palashdas/.local/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/palashdas/.local/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/palashdas/.local/lib/python3.8/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/palashdas/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "Using cached einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: typing-extensions, tqdm, einops, pandas\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.59.0\n",
      "    Uninstalling tqdm-4.59.0:\n",
      "      Successfully uninstalled tqdm-4.59.0\n",
      "  Attempting uninstall: einops\n",
      "    Found existing installation: einops 0.4.1\n",
      "    Uninstalling einops-0.4.1:\n",
      "      Successfully uninstalled einops-0.4.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.5\n",
      "    Uninstalling pandas-1.0.5:\n",
      "      Successfully uninstalled pandas-1.0.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
      "pytorch-lightning 1.6.5 requires protobuf<=3.20.1, but you have protobuf 4.25.3 which is incompatible.\n",
      "tensorflow 2.13.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed einops-0.7.0 pandas-2.0.3 tqdm-4.66.2 typing-extensions-4.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade typing-extensions tqdm decorator pandas einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea50e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/palashdas/.local/lib/python3.8/site-packages (2.13.1)\n",
      "Requirement already satisfied: keras in /home/palashdas/.local/lib/python3.8/site-packages (2.13.1)\n",
      "Collecting keras\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (23.1.21)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (2.2.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (0.30.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/palashdas/.local/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/palashdas/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/palashdas/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/palashdas/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/palashdas/.local/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/palashdas/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/palashdas/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/palashdas/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/palashdas/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/palashdas/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/palashdas/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/palashdas/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.14.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/palashdas/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.1.0)\n",
      "Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: typing-extensions\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sqlalchemy 2.0.26 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pytorch-lightning 1.6.5 requires protobuf<=3.20.1, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed typing-extensions-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904d3c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: typer in /home/palashdas/.local/lib/python3.8/site-packages (0.3.2)\n",
      "Collecting typer\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: click in /home/palashdas/.local/lib/python3.8/site-packages (8.1.3)\n",
      "Collecting click\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/palashdas/.local/lib/python3.8/site-packages (from typer) (4.5.0)\n",
      "Collecting shellingham>=1.3.0 (from typer)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/palashdas/.local/lib/python3.8/site-packages (from typer) (13.7.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/palashdas/.local/lib/python3.8/site-packages (from rich>=10.11.0->typer) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/palashdas/.local/lib/python3.8/site-packages (from rich>=10.11.0->typer) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/palashdas/.local/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer) (0.1.2)\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m420.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: shellingham, click, typer\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.3\n",
      "    Uninstalling click-8.1.3:\n",
      "      Successfully uninstalled click-8.1.3\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.3.2\n",
      "    Uninstalling typer-0.3.2:\n",
      "      Successfully uninstalled typer-0.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.0.9 requires typer<0.4.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed click-8.1.7 shellingham-1.5.4 typer-0.12.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade typer click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "264969e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import string\n",
    "import spacy\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import indian\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input,Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f11e1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/palashdas/.local/lib/python3.8/site-packages (2.13.1)\n",
      "Requirement already satisfied: keras in /home/palashdas/.local/lib/python3.8/site-packages (2.13.1)\n",
      "Collecting keras\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (23.1.21)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorflow) (0.30.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/palashdas/.local/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/palashdas/.local/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/palashdas/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/palashdas/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/palashdas/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/palashdas/.local/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/palashdas/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/palashdas/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/palashdas/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/palashdas/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/palashdas/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/palashdas/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/palashdas/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.14.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/palashdas/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.1.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262c73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./Dataset_English_Hindi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c14bc371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Hindi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Help!</td>\n",
       "      <td>बचाओ!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>उछलो.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>कूदो.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>छलांग.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello!</td>\n",
       "      <td>नमस्ते।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English    Hindi\n",
       "0   Help!    बचाओ!\n",
       "1   Jump.    उछलो.\n",
       "2   Jump.    कूदो.\n",
       "3   Jump.   छलांग.\n",
       "4  Hello!  नमस्ते।"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ddff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 130476 entries, 0 to 130475\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   English  130474 non-null  object\n",
      " 1   Hindi    130164 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "686e4fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English      2\n",
       "Hindi      312\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68275d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf30958",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['English'] = data['English'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3675721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    if isinstance(text,str):\n",
    "\n",
    "        pattern = re.compile('<.*?>')\n",
    "        return pattern.sub(r'',text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "data['English'] = data['English'].apply(remove_html)\n",
    "data['Hindi'] = data['Hindi'].apply(remove_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26b41e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    if isinstance(text,str):\n",
    "        pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        return pattern.sub(r'',text)\n",
    "    else:\n",
    "        return\n",
    "data['English'] = data['English'].apply(remove_url)\n",
    "data['Hindi'] = data['Hindi'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b4da6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, language='english'):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    if language == 'english':\n",
    "        pattern = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "        return pattern.sub(r'', text)\n",
    "    elif language == 'hindi':\n",
    "        pattern = re.compile(r'[^\\u0900-\\u097F\\s]')\n",
    "        return pattern.sub(r'', text)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported Language, Supported languages are 'english' and 'hindi'\")\n",
    "\n",
    "data['English'] = data['English'].apply(lambda x: preprocess_text(x, language='english'))\n",
    "data['Hindi'] = data['Hindi'].apply(lambda x: preprocess_text(x, language='hindi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84abd73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi Punctuation:  ‖‗†‡•‣․‥…‧‰‱′″‴‵‶‷‸※‼‽‾⁁⁂⁃⁇⁈⁉⁊⁋⁌⁍⁎⁏⁐⁑⁓⁕⁖⁗⁘⁙⁚⁛⁜⁝⁞\n"
     ]
    }
   ],
   "source": [
    "def get_hindi_punctuations():\n",
    "    hindi_punctuations = []\n",
    "    for i in range(0x2000, 0x206f + 1):\n",
    "        char = chr(i)\n",
    "        if unicodedata.category(char) == 'Po':\n",
    "            hindi_punctuations.append(char)\n",
    "    return ''.join(hindi_punctuations)\n",
    "\n",
    "# Get Hindi punctuation characters\n",
    "hindi_punctuation = get_hindi_punctuations()\n",
    "\n",
    "# Print Hindi punctuation characters\n",
    "print('Hindi Punctuation: ', hindi_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea52e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text, language = 'English'):\n",
    "    if language == 'English':\n",
    "        exclude_english = set(string.punctuation)\n",
    "        return ''.join(char for char in text if char not in exclude_english)\n",
    "    elif language == 'Hindi':\n",
    "        return ''.join(char for char in text if char not in hindi_punctuation)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported Language, Supported languages are 'english' and 'hindi'\")\n",
    "\n",
    "\n",
    "data['English'] = data['English'].apply(lambda x: remove_punctuation(x,language = 'English'))\n",
    "data['Hindi'] = data['Hindi'].apply(lambda x: remove_punctuation(x,language = 'Hindi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d51d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    expanded_text = contractions.fix(text)\n",
    "    return expanded_text\n",
    "\n",
    "data['English'] = data['English'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8b5cf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58acc7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/palashdas/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "346a000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_tokenization(text):\n",
    "    token_words = word_tokenize(text)\n",
    "    return token_words\n",
    "\n",
    "data['English'] = data['English'].apply(do_tokenization)\n",
    "data['Hindi'] = data['Hindi'].apply(do_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62f142af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/palashdas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package indian to /home/palashdas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/indian.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('indian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cc0180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_english = set(stopwords.words('english'))\n",
    "stop_words_hindi = set(nltk.corpus.indian.words('hindi.pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "871285c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text,language = 'english'):\n",
    "    if language == 'english':\n",
    "        filtered_words_english = [word for word in text if word.lower() not in stop_words_english]\n",
    "        return ' '.join(filtered_words_english)\n",
    "    elif language == 'hindi':\n",
    "        filterd_words_hindi = [word for word in text if word not in stop_words_hindi]\n",
    "        return ' '.join(filterd_words_hindi)\n",
    "    else:\n",
    "        return ValueError(\"Unsupported Language, Supported languages are 'english' and 'hindi'\")\n",
    "\n",
    "data['English'] = data['English'].apply(lambda x :remove_stopwords(x,language = 'english'))\n",
    "data['Hindi'] = data['Hindi'].apply(lambda x :remove_stopwords(x,language = 'hindi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23f5980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_stemming(token_words):\n",
    "    ps = PorterStemmer()\n",
    "    words = token_words.split()\n",
    "    return [ps.stem(word) for word in words]\n",
    "\n",
    "data['English'] = data['English'].apply(do_stemming)\n",
    "data['Hindi'] = data['Hindi'].apply(do_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c74e242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length in combined dataset x:  218\n",
      "Maximum sequence length in combined dataset x:  133\n"
     ]
    }
   ],
   "source": [
    "def calculate_max_sequence_length(text):\n",
    "    # Concatenate x_train_seq and x_test_seq\n",
    "    combined_sequences = text\n",
    "    max_length_combined = max(len(sequence) for sequence in combined_sequences)\n",
    "\n",
    "    return max_length_combined\n",
    "\n",
    "max_length_combined_x = calculate_max_sequence_length(data['English'])\n",
    "print(\"Maximum sequence length in combined dataset x: \", max_length_combined_x)\n",
    "max_length_combined_y = calculate_max_sequence_length(data['Hindi'])\n",
    "print(\"Maximum sequence length in combined dataset x: \", max_length_combined_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed6b94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(data['English'])\n",
    "tok_hindi = Tokenizer()\n",
    "tok_hindi.fit_on_texts(data['Hindi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e45ebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_hindi.word_index['<start>'] = len(tok_hindi.word_index) + 1\n",
    "tok_hindi.word_index['<end>'] = len(tok_hindi.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9289652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens(sequences, start_token='<start>', end_token='<end>'):\n",
    "    sequences_with_special_tokens = []\n",
    "    for sequence in sequences:\n",
    "        sequence_with_special_tokens = [start_token] + sequence + [end_token]\n",
    "        sequences_with_special_tokens.append(sequence_with_special_tokens)\n",
    "    return sequences_with_special_tokens\n",
    "\n",
    "data['Hindi'] = add_special_tokens(data['Hindi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c46314d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['English'] = tok.texts_to_sequences(data['English'])\n",
    "data['Hindi'] = tok_hindi.texts_to_sequences(data['Hindi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67d7fc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length in combined dataset x:  218\n",
      "Maximum sequence length in combined dataset x:  135\n"
     ]
    }
   ],
   "source": [
    "def calculate_max_sequence_length(text):\n",
    "    # Concatenate x_train_seq and x_test_seq\n",
    "    combined_sequences = text\n",
    "    max_length_combined = max(len(sequence) for sequence in combined_sequences)\n",
    "\n",
    "    return max_length_combined\n",
    "\n",
    "max_length_combined_x = calculate_max_sequence_length(data['English'])\n",
    "print(\"Maximum sequence length in combined dataset x: \", max_length_combined_x)\n",
    "max_length_combined_y = calculate_max_sequence_length(data['Hindi'])\n",
    "print(\"Maximum sequence length in combined dataset x: \", max_length_combined_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b376d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "english = data['English']\n",
    "hindi = data['Hindi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb1caadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(english,hindi,test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37222595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:  (104129,) (104129,)\n",
      "Testing set shape:  (26033,) (26033,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set shape: \",x_train.shape,y_train.shape)\n",
    "print(\"Testing set shape: \",x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a002fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_main = pad_sequences(x_train, maxlen=max_length_combined_x, padding='post')\n",
    "y_train_main = pad_sequences(y_train, maxlen=max_length_combined_y, padding='post')\n",
    "x_test_main = pad_sequences(x_test, maxlen=max_length_combined_x, padding='post')\n",
    "y_test_main = pad_sequences(y_test, maxlen=max_length_combined_y, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca195623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train_main is : (104129, 218)\n",
      "shape of y_train_main is : (104129, 135)\n",
      "shape of x_test_main is : (26033, 218)\n",
      "shape of y_test_main is : (26033, 135)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of x_train_main is :\",x_train_main.shape)\n",
    "print(\"shape of y_train_main is :\",y_train_main.shape)\n",
    "print(\"shape of x_test_main is :\",x_test_main.shape)\n",
    "print(\"shape of y_test_main is :\",y_test_main.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "917c2e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size_input, vocab_size_output, max_seq_length_input, max_seq_length_output, embedding_dim, hidden_units):\n",
    "    # Define encoder input layer\n",
    "    encoder_inputs = Input(shape=(max_seq_length_input,))\n",
    "    \n",
    "    # Define encoder embedding layer\n",
    "    encoder_embedding = Embedding(input_dim=vocab_size_input, output_dim=embedding_dim)(encoder_inputs)\n",
    "    \n",
    "    # Define encoder LSTM layer\n",
    "    encoder_lstm = LSTM(hidden_units, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "    \n",
    "    # Discard encoder outputs and keep only the states\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Define decoder input layer\n",
    "    decoder_inputs = Input(shape=(max_seq_length_output,))\n",
    "    \n",
    "    # Define decoder embedding layer\n",
    "    decoder_embedding = Embedding(input_dim=vocab_size_output, output_dim=embedding_dim)(decoder_inputs)\n",
    "    \n",
    "    # Define decoder LSTM layer with initial state set to encoder states\n",
    "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    \n",
    "    # Define decoder output layer\n",
    "    decoder_dense = Dense(vocab_size_output, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7edd2f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size for English: 56739\n",
      "Vocabulary size for Hindi: 75242\n"
     ]
    }
   ],
   "source": [
    "vocab_size_input = len(tok.word_index) + 1  # Add 1 for padding token\n",
    "#max_seq_length_input = max(len(str(sentence)) for sentence in data['English'])\n",
    "\n",
    "vocab_size_output = len(tok_hindi.word_index) + 1  # Add 1 for padding token\n",
    "#max_seq_length_output = max(len(str(sentence)) for sentence in data['Hindi'])\n",
    "\n",
    "print(\"Vocabulary size for English:\", vocab_size_input)\n",
    "#print(\"Maximum sequence length for English:\", max_seq_length_input)\n",
    "print(\"Vocabulary size for Hindi:\", vocab_size_output)\n",
    "#print(\"Maximum sequence length for Hindi:\", max_seq_length_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "55be9ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length in combined english dataset:  218\n",
      "Maximum sequence length in combined hindi dataset:  134\n"
     ]
    }
   ],
   "source": [
    "def calculate_max_sequence_length(text):\n",
    "    # Concatenate x_train_seq and x_test_seq\n",
    "    combined_sequences = text\n",
    "    max_length_combined = max(len(sequence) for sequence in combined_sequences)\n",
    "\n",
    "    return max_length_combined\n",
    "\n",
    "max_seq_length_input = calculate_max_sequence_length(data['English'])\n",
    "print(\"Maximum sequence length in combined english dataset: \", max_seq_length_input)\n",
    "max_seq_length_output = calculate_max_sequence_length(data['Hindi']) - 1\n",
    "print(\"Maximum sequence length in combined hindi dataset: \", max_seq_length_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a4c4b725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)        [(None, 218)]                0         []                            \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)       [(None, 134)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)     (None, 218, 100)             5673900   ['input_9[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)     (None, 134, 100)             7524200   ['input_10[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_8 (LSTM)               [(None, 256),                365568    ['embedding_8[0][0]']         \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " lstm_9 (LSTM)               [(None, 134, 256),           365568    ['embedding_9[0][0]',         \n",
      "                              (None, 256),                           'lstm_8[0][1]',              \n",
      "                              (None, 256)]                           'lstm_8[0][2]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 134, 75242)           1933719   ['lstm_9[0][0]']              \n",
      "                                                          4                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 33266430 (126.90 MB)\n",
      "Trainable params: 33266430 (126.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_units = 256\n",
    "batch_size = 32\n",
    "\n",
    "model = create_model(vocab_size_input, vocab_size_output, max_seq_length_input, max_seq_length_output, embedding_dim, hidden_units)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66d9b177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "3255/3255 [==============================] - 451s 138ms/step - loss: 0.5101 - accuracy: 0.9537 - val_loss: 0.4409 - val_accuracy: 0.9545\n",
      "Epoch 2/6\n",
      "3255/3255 [==============================] - 422s 130ms/step - loss: 0.4234 - accuracy: 0.9547 - val_loss: 0.4179 - val_accuracy: 0.9549\n",
      "Epoch 3/6\n",
      "3255/3255 [==============================] - 421s 129ms/step - loss: 0.3885 - accuracy: 0.9554 - val_loss: 0.3971 - val_accuracy: 0.9556\n",
      "Epoch 4/6\n",
      "3255/3255 [==============================] - 422s 130ms/step - loss: 0.3514 - accuracy: 0.9572 - val_loss: 0.3795 - val_accuracy: 0.9573\n",
      "Epoch 5/6\n",
      "3255/3255 [==============================] - 420s 129ms/step - loss: 0.3151 - accuracy: 0.9599 - val_loss: 0.3651 - val_accuracy: 0.9594\n",
      "Epoch 6/6\n",
      "3255/3255 [==============================] - 419s 129ms/step - loss: 0.2828 - accuracy: 0.9624 - val_loss: 0.3574 - val_accuracy: 0.9610\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=3, monitor='val_loss'),  # Stop training if validation loss stops improving for 3 epochs\n",
    "    ModelCheckpoint(filepath='model_weights.weights.h5', save_best_only=True, save_weights_only=True, monitor='val_loss')  # Save the model with the best validation loss\n",
    "]\n",
    "\n",
    "# Fit the model with callbacks\n",
    "history = model.fit(x=[x_train_main, y_train_main[:, :-1]],  # Exclude last token from decoder input\n",
    "                    y=y_train_main[:, 1:],  # Exclude first token from decoder target\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=6,\n",
    "                    validation_data=([x_test_main, y_test_main[:, :-1]], y_test_main[:, 1:]),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e38649a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved successfully.\n"
     ]
    }
   ],
   "source": [
    "weights_file_path = './model_weights_main.weights.h5'\n",
    "\n",
    "model.save_weights(weights_file_path)\n",
    "\n",
    "print(\"Model weights saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5c0cf48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29085c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3eed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ff2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
